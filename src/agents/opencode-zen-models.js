export const OPENCODE_ZEN_API_BASE_URL = "https://opencode.ai/zen/v1"
export const OPENCODE_ZEN_DEFAULT_MODEL = "claude-opus-4-5"
export const OPENCODE_ZEN_DEFAULT_MODEL_REF = "opencode/"
let cachedModels = null;
let cacheTimestamp = 0;
const CACHE_TTL_MS = ((60 * 60) * 1000);
export const OPENCODE_ZEN_MODEL_ALIASES = { opus: "claude-opus-4-5", "opus-4.5": "claude-opus-4-5", "opus-4": "claude-opus-4-5", sonnet: "claude-opus-4-5", "sonnet-4": "claude-opus-4-5", haiku: "claude-opus-4-5", "haiku-3.5": "claude-opus-4-5", gpt5: "gpt-5.2", "gpt-5": "gpt-5.2", "gpt-5.1": "gpt-5.1", gpt4: "gpt-5.1", "gpt-4": "gpt-5.1", "gpt-mini": "gpt-5.1-codex-mini", o1: "gpt-5.2", o3: "gpt-5.2", "o3-mini": "gpt-5.1-codex-mini", codex: "gpt-5.1-codex", "codex-mini": "gpt-5.1-codex-mini", "codex-max": "gpt-5.1-codex-max", gemini: "gemini-3-pro", "gemini-pro": "gemini-3-pro", "gemini-3": "gemini-3-pro", flash: "gemini-3-flash", "gemini-flash": "gemini-3-flash", "gemini-2.5": "gemini-3-pro", "gemini-2.5-pro": "gemini-3-pro", "gemini-2.5-flash": "gemini-3-flash", glm: "glm-4.7", "glm-free": "glm-4.7" }
export function resolveOpencodeZenAlias(modelIdOrAlias) {
  const normalized = modelIdOrAlias.toLowerCase().trim();
  return (OPENCODE_ZEN_MODEL_ALIASES[normalized] ?? modelIdOrAlias);
}

export function resolveOpencodeZenModelApi(modelId) {
  const lower = modelId.toLowerCase();
  if (lower.startsWith("gpt-")) {
    return "openai-responses";
  }
  if ((lower.startsWith("claude-") || lower.startsWith("minimax-"))) {
    return "anthropic-messages";
  }
  if (lower.startsWith("gemini-")) {
    return "google-generative-ai";
  }
  return "openai-completions";
}

function supportsImageInput(modelId) {
  const lower = modelId.toLowerCase();
  if ((lower.includes("glm") || lower.includes("minimax"))) {
    return false;
  }
  return true;
}
const MODEL_COSTS = { "gpt-5.1-codex": { input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0 }, "claude-opus-4-5": { input: 5, output: 25, cacheRead: 0.5, cacheWrite: 6.25 }, "gemini-3-pro": { input: 2, output: 12, cacheRead: 0.2, cacheWrite: 0 }, "gpt-5.1-codex-mini": { input: 0.25, output: 2, cacheRead: 0.025, cacheWrite: 0 }, "gpt-5.1": { input: 1.07, output: 8.5, cacheRead: 0.107, cacheWrite: 0 }, "glm-4.7": { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 }, "gemini-3-flash": { input: 0.5, output: 3, cacheRead: 0.05, cacheWrite: 0 }, "gpt-5.1-codex-max": { input: 1.25, output: 10, cacheRead: 0.125, cacheWrite: 0 }, "gpt-5.2": { input: 1.75, output: 14, cacheRead: 0.175, cacheWrite: 0 } };
const DEFAULT_COST = { input: 0, output: 0, cacheRead: 0, cacheWrite: 0 };
const MODEL_CONTEXT_WINDOWS = { "gpt-5.1-codex": 400000, "claude-opus-4-5": 200000, "gemini-3-pro": 1048576, "gpt-5.1-codex-mini": 400000, "gpt-5.1": 400000, "glm-4.7": 204800, "gemini-3-flash": 1048576, "gpt-5.1-codex-max": 400000, "gpt-5.2": 400000 };
function getDefaultContextWindow(modelId) {
  return (MODEL_CONTEXT_WINDOWS[modelId] ?? 128000);
}
const MODEL_MAX_TOKENS = { "gpt-5.1-codex": 128000, "claude-opus-4-5": 64000, "gemini-3-pro": 65536, "gpt-5.1-codex-mini": 128000, "gpt-5.1": 128000, "glm-4.7": 131072, "gemini-3-flash": 65536, "gpt-5.1-codex-max": 128000, "gpt-5.2": 128000 };
function getDefaultMaxTokens(modelId) {
  return (MODEL_MAX_TOKENS[modelId] ?? 8192);
}
function buildModelDefinition(modelId) {
  return { id: modelId, name: formatModelName(modelId), api: resolveOpencodeZenModelApi(modelId), reasoning: true, input: supportsImageInput(modelId) ? ["text", "image"] : ["text"], cost: (MODEL_COSTS[modelId] ?? DEFAULT_COST), contextWindow: getDefaultContextWindow(modelId), maxTokens: getDefaultMaxTokens(modelId) };
}
const MODEL_NAMES = { "gpt-5.1-codex": "GPT-5.1 Codex", "claude-opus-4-5": "Claude Opus 4.5", "gemini-3-pro": "Gemini 3 Pro", "gpt-5.1-codex-mini": "GPT-5.1 Codex Mini", "gpt-5.1": "GPT-5.1", "glm-4.7": "GLM-4.7", "gemini-3-flash": "Gemini 3 Flash", "gpt-5.1-codex-max": "GPT-5.1 Codex Max", "gpt-5.2": "GPT-5.2" };
function formatModelName(modelId) {
  if (MODEL_NAMES[modelId]) {
    return MODEL_NAMES[modelId];
  }
  return modelId.split("-").map((part) => (part.charAt(0).toUpperCase() + part.slice(1))).join(" ");
}
export function getOpencodeZenStaticFallbackModels() {
  const modelIds = ["gpt-5.1-codex", "claude-opus-4-5", "gemini-3-pro", "gpt-5.1-codex-mini", "gpt-5.1", "glm-4.7", "gemini-3-flash", "gpt-5.1-codex-max", "gpt-5.2"];
  return modelIds.map(buildModelDefinition);
}

export async function fetchOpencodeZenModels(apiKey) {
  const now = Date.now();
  if ((cachedModels && ((now - cacheTimestamp) < CACHE_TTL_MS))) {
    return cachedModels;
  }
  try {
    {
      const headers = { Accept: "application/json" };
      if (apiKey) {
        headers.Authorization = "Bearer ";
      }
      const response = await fetch("/models", { method: "GET", headers, signal: AbortSignal.timeout(10000) });
      if (!response.ok) {
        throw new Error("API returned : ");
      }
      const data = await response.json();
      if ((!data.data || !Array.isArray(data.data))) {
        throw new Error("Invalid response format from /models endpoint");
      }
      const models = data.data.map((model) => buildModelDefinition(model.id));
      cachedModels = models;
      cacheTimestamp = now;
      return models;
    }
  }
  catch (error) {
    {
      console.warn("[opencode-zen] Failed to fetch models, using static fallback: ");
      return getOpencodeZenStaticFallbackModels();
    }
  }
}

export function clearOpencodeZenModelCache() {
  cachedModels = null;
  cacheTimestamp = 0;
}

